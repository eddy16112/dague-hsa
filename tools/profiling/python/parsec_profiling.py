#!/usr/bin/env python

""" PURE PYTHON interface to the PaRSEC Python Profiles generated by parsec_binprof.

The recommended shorthand for the name is "P3",
as in the 3 Ps of the alternative abbreviation "PPP".
Therefore, the preferred import method is "import parsec_profiling as p3"

This module is especially suitable for use when a separate Python program
has done the reading of the profile and has stored the profile in this format
in some sort of cross-process format. The format natively supported by pandas
and P3 is HDF5, and can be easily written to and read from using the functions
to_hdf and from_hdf.
"""

import sys
import os
import re
import shutil
import numpy as np
import pandas as pd

import warnings # because these warnings are annoying, and I can find no way around them.
warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)
warnings.simplefilter(action = "ignore", category = FutureWarning)

p3_core = '.h5-'

default_descriptors = ['hostname', 'exe', 'ncores', 'sched']

class ParsecProfile(object):
    class_version = 1.0 # created 2013.10.22 after move to pandas
    basic_event_columns = ['node_id', 'thread_id',  'handle_id', 'type',
                           'begin', 'end', 'duration', 'flags', 'id']
    HDF_TOP_LEVEL_NAMES = ['event_types', 'event_names', 'event_attributes',
                           'nodes', 'threads', 'information', 'errors']

    # the init function should not ordinarily be used
    # it is better to use from_hdf(), from_native(), or autoload()
    def __init__(self, events, event_types, event_names, event_attributes,
                 nodes, threads, information, errors):
        self.__version__ = self.__class__.class_version
        # core data
        self.events = events
        self.event_types = event_types
        self.event_names = event_names
        self.event_attributes = event_attributes
        self.nodes = nodes
        self.threads = threads
        self.information = information
        self.errors = errors
        # metadata
        self.basic_columns = ParsecProfile.basic_event_columns

    def to_hdf(self, filename, table=False, append=False, complevel=0, complib='blosc'):
        store = pd.HDFStore(filename + '.tmp', 'w')
        for name in ParsecProfile.HDF_TOP_LEVEL_NAMES:
            store.put(name, self.__dict__[name])
        store.put('events', self.events, table=table, append=append)
        store.close()
        # do atomic move once it's finished writing,
        # so as to allow Ctrl-Cs without secret breakage
        shutil.move(filename + '.tmp', filename)

    # this allows certain 'acceptable' attribute abbreviations
    # and automatically searches the 'information' dictionary
    def __getattr__(self, name):
        try:
            return nice_val(self.information, raw_key(self.information, name))
        except:
            return object.__getattribute__(self, name)

    def __repr__(self):
        return describe_dict(self.information)

    def name(self, infos=default_descriptors, add_infos=None):
        if not infos:
            infos = []
        if add_infos:
            infos += add_infos
        return describe_dict(self.information, keys=infos, sep='_')

    def unique_name(self, add_infos=None):
        infos = default_descriptors + ['start_time']
        return self.name(infos=infos, add_infos=add_infos)

    # use with care - does an eval() on self'user text' when 'user text' starts with '.'
    def filter_events(self, filter_strings):
        events = self.events
        for filter_str in filter_strings:
            key, value = filter_str.split('==')
            if str(value).startswith('.'):
                # do eval
                eval_str = 'self' + str(value)
                value = eval(eval_str)
            events = events[:][events[key] == value]
        return events

    def close(self):
        try:
            self._store.close()
        except:
            pass

    def __del__(self):
        self.close()


def raw_key(dict_, name):
    """ Converts a simple key name into the actual key name by PaRSEC rules."""
    try:
        dict_[name]
        return name
    except:
        pass
    try:
        dict_[str(name).upper()]
        return str(name).upper()
    except:
        pass
    try:
        dict_['PARAM_' + str(name).upper()]
        return 'PARAM_' + str(name).upper()
    except:
        return name


def from_hdf(filename, skeleton_only=False, keep_store=False):
    """ Loads a PaRSEC profile from an existing HDF5 format file."""
    store = pd.HDFStore(filename, 'r')
    top_level = list()
    if not skeleton_only:
        events = store['events']
    else:
        events = pd.DataFrame()
    try:
        for name in ParsecProfile.HDF_TOP_LEVEL_NAMES:
            top_level.append(store[name])
    except KeyError as ke:
        print(ke)
        print(store['information'])
        raise ke
    profile = ParsecProfile(events, *top_level)
    if keep_store:
        profile._store = store
    else:
        store.close()
    return profile


def find_profile_sets(profiles, on=['cmdline']): #['N', 'M', 'NB', 'MB', 'IB', 'sched', 'exe', 'hostname'] ):
    profile_sets = dict()
    for profile in profiles:
        name = ''
        for info in on:
            name += str(profile.__getattr__(info)).replace('/', '') + '_'
        try:
            name = name[:-1]
            profile_sets[name].append(profile)
        except:
            profile_sets[name] = [profile]
    return profile_sets

# Does a best-effort merge on sets of profiles.
# Merges only the events, threads, and nodes, along with
# the top-level "information" struct.
# Intended for use after 'find_profile_sets'
# dangerous for use with groups of profiles that do not
# really belong to a reasonably-defined set.
# In particular, the event_type, event_name, and event_attributes
# DataFrames are chosen from the first profile in the set - no
# attempt is made to merge them at this time.
def automerge_profile_sets(profile_sets):
    merged_profiles = list()
    for p_set in profile_sets:
        merged_profile = p_set[0]
        for profile in p_set[1:]:
            # ADD UNIQUE ID
            #
            # add start time as id to every row in events and threads DataFrames
            # so that it is still possible to 'split' the merged profile
            # based on start_time id, which should differ for every run...
            if profile == p_set[1]:
                start_time_array = np.empty(len(merged_profile.events), dtype=int)
                start_time_array.fill(merged_profile.start_time)
                merged_profile.events['start_time'] = pd.Series(start_time_array)
                merged_profile.threads['start_time'] = pd.Series(
                    start_time_array[:len(merged_profile.threads)])
            start_time_array = np.empty(len(profile.events), dtype=int)
            start_time_array.fill(profile.start_time)
            events = profile.events
            events['start_time'] = pd.Series(start_time_array)
            threads = profile.threads
            threads['start_time'] = pd.Series(start_time_array[:len(threads)])
            # CONCATENATE EVENTS
            merged_profile.events = pd.concat([merged_profile.events, events])
            merged_profile.nodes = pd.concat([merged_profile.nodes, profile.nodes])
            merged_profile.threads = pd.concat([merged_profile.threads, threads])
        merged_profile.information = match_dicts([profile.information for profile in p_set])
        merged_profiles.append(merged_profile)
    return merged_profiles

def match_dicts(dicts):
    """ Returns the matching or compatible parts of multi-type dictionaries.

    Only matching keys and values will be retained, except:
    Matching keys with float values will be averaged.

    Retains the actual type of the items passed, assuming they are
    all the same type of dictionary-like object."""

    if len(dicts) == 0:
        return dict()

    matched_info = dicts[0]
    mult = 1.0 / len(dicts)
    for dict_ in dicts[1:]:
        for key, value in dict_.iteritems():
            if key not in matched_info: # not present
                matched_info.drop(key)
            elif value != matched_info[key]:
                try:
                    temp_fl = float(value)
                    if '.' in str(value): # if number was actually a float
                        # do average
                        if profile == p_set[1]:
                            matched_info[key] = matched_info[key] * mult
                        matched_info[key] += value * mult
                    else: # not float
                        matched_info.drop(key)
                except: # not equal and not float
                    matched_info.drop(key)
    return matched_info


def describe_dict(dict_, keys=default_descriptors, sep=' ', key_val_sep=None):
    description = str()
    used_keys = []
    for key in keys:
        real_key = raw_key(dict_, key)
        try:
            if real_key in used_keys:
                continue # exclude duplicates
            used_keys.append(real_key)
            value = str(nice_val(dict_, real_key))
            if key_val_sep is not None:
                description += str(key) + key_val_sep
            description += value + sep
        except KeyError as e:
            print(e, real_key)
            pass # key doesn't exist - just ignore
    return description[:-len(sep)] # remove last 'sep'

def nice_val(dict_, key):
    """ Edits return values for common usage."""
    if key == 'exe':
        m = re.match('.*testing_(\w+)', dict_[key])
        return m.group(1)
    else:
        return dict_[key]
